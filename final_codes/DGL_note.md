### **取边特征的时候，这是取0、3边的特征**
print(g.edata['x'][th.tensor([0, 3])] )  # 获取边0和3的特征

### **构建图的关键**
- 弄到边关系 包括源和目的 最好是两列
- 弄到节点特征  

有以上两个过程，只要能正确对应就没问题了。  
相比较而言HAR的图是比较好构建的，而SNS的图通常可能比较大，就不容易构建。对于节点属性通常要求是特征矩阵，这就要求在构建过程中不断把节点、边的各种属性转换成各自不同维度的等长特征向量。

HAR在构建图的时候别直接构建，还是从csv文件构建比较好。这样从通道往下查，传感器对应1、2、3、4...，根据传感器分布分配完节点号后回头构建图更为方便。就可以方便的构图了。

从外部文件构建图
[csv文件构建俱乐部图](https://github.com/dglai/WWW20-Hands-on-Tutorial/blob/master/basic_tasks/1_load_data.ipynb)  
csv文件构建图的时候可以参考

dgl.save_graphs()  
dgl.load_graphs()  
数据处理完可以直接以图形式保存，如果处理速度快就无所谓了，如果慢的话可能就得一次处理完以图形式存储。

### __DGL GPU__
首先构建图对象，g.to("cuda:0")同样方式送到gpu上，特征也被转移到了gpu上
也可先将源和目标节点转换到GPU上，下一步构建的图自动在gpu上。

### __下一步还要考虑异质图构建__
异质图构建

DGL的异质图构建较为方便，在做HAR对比实验的时候可以采用异质图-》同质体对比的形式来看实验对比结果。  
而且在节点特征数据当中由于在采集的时候已经按照所有的传感器采样频率相同处理，因此更容易转换成同质图来进行处理。



### __消息传递 重点__

__1.__ 边权重是首先要计算的，有上一层的节点和边值来计算  
__2.__ 根据边权重来送入聚合函数，邻居节点的值、边值做汇聚  
__3.__ 聚合的邻居信息、自身的特征信息计算新的节点特征做更新。

假设节点 v 上的的特征为 xv∈Rd1，边 (u,v) 上的特征为 we∈Rd2。 消息传递范式 定义了以下逐节点和边上的计算：

边上计算: m(t+1)e=ϕ(x(t)v,x(t)u,w(t)e),(u,v,e)∈E.
点上计算: x(t+1)v=ψ(x(t)v,ρ({m(t+1)e:(u,v,e)∈E})).
在上面的等式中， ϕ 是定义在每条边上的消息函数，它通过将边上特征与其两端节点的特征相结合来生成消息。 聚合函数 ρ 会聚合节点接受到的消息。 更新函数 ψ 会结合聚合后的消息和节点本身的特征来更新节点的特征。

[原始公式链接](https://docs.dgl.ai/guide_cn/message.html)

### __消息传递 理解总结__
__1. 消息函数：__ 接收edges，为EdgeBatch实例，接收一批边，代表要根据此对象中的所有边逐次进行更新，更新的时候要用到源节点、目的节点、原有数据值，都可以直接从edges中直接提取。此步骤是以所有的边为核心计算对象。
__2. 聚合函数：__ 接收nodes，为NodeBatch实例，接收一批点，代表要根据此对象中的所有点逐次进行更新，更新的时候要用到邻居

__3. 更新函数：__


